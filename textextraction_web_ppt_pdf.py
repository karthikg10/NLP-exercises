# -*- coding: utf-8 -*-
"""TextExtractionFa24_web_ppt_pdf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ezggiRJTi2hgtOxU-zA8c_DjTdaKMjIz

# Part 1 Web Scraping
"""

from google.colab import drive
drive.mount('/content/drive')

"""Web Scraping is an art where one has to study the website and work according to the dynamics of that particular website.

Most common tools used for web scraping in python are demonstrated below.

1. requests https://requests.readthedocs.io/en/latest/
2. beautiful soup https://beautiful-soup-4.readthedocs.io/en/latest/
3. Selenium https://selenium-python.readthedocs.io/
4. Scrapy https://docs.scrapy.org/en/latest/

We will be working on the first three and the fourth one can be explored in the homeworks.

We will be scraping 4 websites today:

1. ScrapeThisSite
2. GeeksforGeeks
3. CNBC
4. Hoopshype

There are different techniques to be used when scraping a dynamic website vs a static website which will be discussed in the coming sections

Some websites have their APIs open and those can be used to directly fetch the data without the need of scraping the HTML or XML pages.
"""

# installing the libraries
!pip install requests
!pip install bs4
!pip install selenium
!apt-get update
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin

# importing the libraries
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import pandas as pd
import json
from google.colab import drive
import sys

url = 'https://www.scrapethissite.com/pages/forms/'

page = requests.get(url)

soup = BeautifulSoup(page.text, 'html')

print(soup)

soup.find('div')

soup.find_all('p')

soup.find('p', class_ = 'lead').text

soup.find('p', class_ = 'lead').text.strip()

soup.find_all('th')

soup.find('th').text.strip()

# getting the first URL
# open the URL in parallel in other tab to check the information we are extracting
url = "https://www.geeksforgeeks.org/python-programming-language/"

# hitting the url and getting the response
res = requests.get(url)
print(res.status_code)

# creating a soup object from the returned html page
sp = BeautifulSoup(res.text, "lxml")
sp

# printing it in readable format
print(sp.prettify())

# parsing title element of the page
print(sp.title)
print(sp.title.name)
print(sp.title.string)
print(sp.title.parent.name)

# extracting the title of the article
print(sp.find("div", {"class" : "article-title"}).text)

print(sp.find("div"))

# extracting the date of the article
date_label = sp.find('span', class_='strong', text='Last Updated : ')
date_span = date_label.find_next('span')
last_updated_date = date_span.text
print(f"Last Updated Date: {last_updated_date}")

# extracting the content of the article
# it extracts everyhting together, in the next sections we can see how to iteratively extract information paragraph by paragraph
print(sp.find("div", {"class" : "text"}).text)

# extracting the links found in the bottom of the article for further reading
for tag in sp.find("div", {"class":"gfg-similar-reads-list"}).findAll("a", href = True): print(tag.text, "\n", tag["href"], "\n\n")

# extracting the links found in the bottom of the article for further reading
for link in sp.find('div', class_='gfg-similar-reads-list').find_all('a'):
    print(link.get('href'))

# extracting information paragraph by paragraph
for tag in sp.find("div", {"class":"text"}).findAll("p"): print(tag.text)

"""Almost all the geeksforgeeks articles have the same format and hence all of them can be scraped using the same code, this repeatability is useful while doing web scraping as a block of code can help one get a lot of information in a structured way

Other information can be extracted using similar methods as used in geeksforgeeks, this can be done in homework

Again same as geeksforgeeks, all articles of scrapethissite
are similar in structure and hence the same code can be used to scrap through all the articles of this website

Now working with a dynamic website that has its API open.

Open the CNBC website and search for any topic. If we search for SPORTS the URL looks like this: https://www.cnbc.com/search/?query=SPORTS&qsearchterm=SPORTS and if we search for POLITICS the URL looks like this: https://www.cnbc.com/search/?query=POLITICS&qsearchterm=POLITICS

Here we can observe a pattern and we can predict what the url would look like if we search something else, this information can be used for reusability and repseatability of code.

Now we can see that there are more than 50,000 results for the topic politics but only 10 are loaded in the beginning. Once we scroll down, next 10 are loaded and so on. To load the next results when the user scrolls down, an API is hit and link to that API is found from the network section of the inspect element: https://api.queryly.com/cnbc/json.aspx?queryly_key=31a35d40a9a64ab3&query=POLITICS&endindex=10&batchsize=10&callback=&showfaceted=false&timezoneoffset=420&facetedfields=formats&facetedkey=formats%7C&facetedvalue=!Press%20Release%7C&additionalindexes=4cd6f71fbf22424d,937d600b0d0d4e23,3bfbe40caee7443e,626fdfcd96444f28

Here playing around with the endindex and batchsize parameter we can get various results.

Now the response of this API call would be a JSON and hence the need for parsing a web page is gone when there is an open API
"""

# getting the third url
url = "https://api.queryly.com/cnbc/json.aspx?queryly_key=31a35d40a9a64ab3&query=POLITICS&endindex=10&batchsize=10&callback=&showfaceted=false&timezoneoffset=420&facetedfields=formats&facetedkey=formats%7C&facetedvalue=!Press%20Release%7C&additionalindexes=4cd6f71fbf22424d,937d600b0d0d4e23,3bfbe40caee7443e,626fdfcd96444f28"

# hitting the url and getting the response
res = requests.get(url)
print(res.status_code)

res.text

# getting the description of each news article
for description in json.loads(res.text)["results"]: print(description["description"], "\n")

"""Finally working with Selenium"""

!echo | sudo add-apt-repository ppa:saiarcot895/chromium-beta
!sudo apt remove chromium-browser
!sudo snap remove chromium
!sudo apt install chromium-browser -qq
# Chromium (an open-source version of Chrome) and Chromium WebDriver (which allows Selenium to control Chromium).

!pip3 install selenium --quiet
!apt-get update
!apt install chromium-chromedriver -qq
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
#Selenium requires a browser driver (in this case, chromedriver) to communicate with the browser. You're installing it using the chromium-chromedriver package and copying it to /usr/bin for easy access.

!pip install --upgrade selenium

!pip install selenium
!apt-get update
!apt-get install -y chromium-chromedriver

import sys
from selenium.webdriver.chrome.service import Service as ChromeService
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
# download the selenium chromedriver executable file and paste the link in the following code
# this code should open a new chrome window in your machine
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_service = ChromeService(
    executable_path='/usr/lib/chromium-browser/chromedriver',
    log_path='/dev/null'  # You can change the log path as needed
)
driver = webdriver.Chrome(service=chrome_service,options=chrome_options)
#The ChromeService class sets up the path to the chromedriver

# this code should open hoopshype website in your newly opened chrome window
driver.get('https://hoopshype.com/salaries/players/')
# After setting up the Selenium WebDriver, the browser (in headless mode) navigates to HoopsHype Salaries page

# getting players name list
players = driver.find_elements("xpath", '//td[@class="name"]')
#uses an XPath selector to find all the HTML elements on the page that have a class attribute "name", which corresponds to the player names.

players_list = []
for p in range(len(players)): players_list.append(players[p].text)
players_list

"""Similarly other information such as players' salaries can also be easily extracted and can be done as homework

# Part 2: PPT Scraping
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install python-pptx

"""Extracting text from PPT using the python-pptx library, it is typically used for generating ppts from databases but we can exploit some of its features here to extract text from ppts, this is a very basic example and it can be explored further as per the need. documentation to the libary: https://python-pptx.readthedocs.io/en/latest/"""

# importing library
from pptx import Presentation

# extracting texts slide wise and section wise
# open the PPT in parallel to check the outcome
prs = Presentation("/content/drive/MyDrive/Your big idea.pptx")
counter_slide = 1
for slide in prs.slides:
    print("slide:", counter_slide, "\n")
    counter_content = 1
    for shape in slide.shapes:
        try:
            print("content:", counter_content, shape.text, "\n")
            counter_content += 1
        except: continue
    print("\n\n")
    counter_slide += 1

"""Similarly other components of the PPT can be extracted after following the documentation as per need

# Part 3: PDF Scraping

Using the library PyPDF2: https://pypi.org/project/PyPDF2/
This library can only extract text from PDFs, for tables and images other methods are required.
Extracting text from PDFs is much difficult compared to web and ppt as there is no inherent structure where just calling the right elements will give us everything, infact pdfs can be seen as an image and hence whatever extraction we do is by using some kind of optical character recognition.
"""

# installing the library
!pip install PyPDF2

# importing the library
import PyPDF2

# reading the file
pdfFileObj = open('/content/drive/MyDrive/Evaluation_of_Sentiment_Analysis_in_Finance_From_Lexicons_to_Transformers.pdf', 'rb')

# passing the file to PyPDF
pdfReader = PyPDF2.PdfReader(pdfFileObj)

# getting the number of pages
print(len(pdfReader.pages))

# getting the first page
pageObj = pdfReader.pages[1]

# extracting text from the first page
print(pageObj.extract_text())

"""# Homework

1. As discussed in the demo above, using the example of geeksforgeeks extract the information from ScrapethisSite articles apart from those that is already demonstrated.

2. As discussed in the demo above, extract the salaries of each of the players from the hoopshype website using the example of how to extract the names.

3. Apart from that choose any 2 websites of your choice and extract meaningful and structured information from there.

4. Also explore the scrapy library to perform webscraping apart from the three discussed above in the demo

5. Pick a website that has tabular data and try to scrap it using the tools studied during the demo.

(The datasets you will be collecting for the projects would be by text extraction so make sure to extract usable structured information)

6. Extract table from a PPT using the same library.

Question 1
"""

url = "https://www.scrapethissite.com/pages/forms/?page_num=10"

res = requests.get(url)

sp = BeautifulSoup(res.text, "lxml")
sp

print(sp.prettify())

print(sp.title)
print(sp.title.name)
print(sp.title.string)
print(sp.title.parent.name)

print(sp.find("title").text)

print(sp.find("td",{"class": "name"}).text)

# table = sp.find("table")

tables = pd.read_html(res.content)

# Check how many tables were found
print(f"Number of tables found: {len(tables)}")

# Display the first table
df = tables[0]
df.head()

for tag in sp.find("ul",{"class" : "pagination"}).findAll("a", href = True): print(tag.text, "\n", tag["href"], "\n\n")

for tag in sp.find("table", {"class":"table"}).findAll("td",{"class" : ["name", "pct text-success"]}): print(tag.text)

"""Question 2"""

driver.get("https://hoopshype.com/salaries/players/")

salaries = driver.find_elements("xpath", '//td[@class="hh-salaries-sorted"]')

player_sal = []
for s in range(len(salaries)): player_sal.append(salaries[s].text)
player_sal

"""Question 3"""

url = "https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States"

res = requests.get(url)
print(res.status_code)

sp = BeautifulSoup(res.text, "lxml")
sp

print(sp.prettify())

sp.find("title")

count = 0
for tag in sp.find("table").findAll("th"):
  if(count<7):
    print(tag.text)
    count += 1

for tag in sp.find("table").findAll("td"): print(tag.text)

tables = pd.read_html(res.content)

# Check how many tables were found
print(f"Number of tables found: {len(tables)}")

# Display the first table
df = tables[0]
df.head()

url = 'https://www.ebay.com/sch/i.html?_nkw=laptops'
response = requests.get(url)

sp = BeautifulSoup(response.content, 'html.parser')

items = sp.find_all('li', {'class': 's-item'})

product_names = []
prices = []
product_links = []

for item in items:
    # Extract product name
    name = item.find('h3', {'class': 's-item__title'})
    if name:
        product_names.append(name.get_text())
    else:
        product_names.append(None)  # Handle missing name

    # Extract product price
    price = item.find('span', {'class': 's-item__price'})
    if price:
        prices.append(price.get_text())
    else:
        prices.append(None)  # Handle missing price

    # Extract product link
    link = item.find('a', {'class': 's-item__link'})
    if link:
        product_links.append(link['href'])
    else:
        product_links.append(None)  # Handle missing link

print(f"Product Names: {len(product_names)}, Prices: {len(prices)}, Product Links: {len(product_links)}")

# Step 6: Create a DataFrame for the extracted data
ebay_df = pd.DataFrame({
    'Product Name': product_names,
    'Price': prices,
    'Product Link': product_links
})

# Step 7: Display the first few rows
ebay_df.head()

"""Question 4"""

!pip install scrapy crochet

import scrapy
from scrapy.crawler import CrawlerProcess
# text cleaning
import re

class QuotesToCsv(scrapy.Spider):
    """scrape first line of  quotes from `wikiquote` by
    Maynard James Keenan and save to json file"""
    name = "MJKQuotesToCsv"
    start_urls = [
        'https://en.wikiquote.org/wiki/Maynard_James_Keenan',
    ]
    custom_settings = {
        'ITEM_PIPELINES': {
            '__main__.ExtractFirstLine': 1
        },
        'FEEDS': {
            'quotes.csv': {
                'format': 'csv',
                'overwrite': True
            }
        }
    }

    def parse(self, response):
        """parse data from urls"""
        for quote in response.css('div.mw-parser-output > ul > li'):
            yield {'quote': quote.extract()}

class ExtractFirstLine(object):
    def process_item(self, item, spider):
        """text processing"""
        lines = dict(item)["quote"].splitlines()
        first_line = self.__remove_html_tags__(lines[0])

        return {'quote': first_line}

    def __remove_html_tags__(self, text):
        """remove html tags from string"""
        html_tags = re.compile('<.*?>')
        return re.sub(html_tags, '', text)

process = CrawlerProcess()
process.crawl(QuotesToCsv)
process.start()

"""Question 5"""

url = 'https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)'
response = requests.get(url)

sp = BeautifulSoup(response.content, 'html.parser')

table = sp.find('table', {'class': 'wikitable sortable'})

# Look for all tables and see what is returned
tables = sp.find_all('table')
print(tables)

# Try reading all tables on the page
tables = pd.read_html(response.content)

# Check how many tables were found
print(f"Number of tables found: {len(tables)}")

# Display the first table
df = tables[0]
df.head()

!pip install python-pptx

"""Question 6"""

# importing library
from pptx import Presentation

# extracting texts slide wise and section wise
# open the PPT in parallel to check the outcome
prs = Presentation("/content/drive/MyDrive/Introduction-to-Data-Science.pptx")
counter_slide = 1
for slide in prs.slides:
    print("slide:", counter_slide, "\n")
    counter_content = 1
    for shape in slide.shapes:
        try:
            print("content:", counter_content, shape.text, "\n")
            counter_content += 1
        except: continue
    print("\n\n")
    counter_slide += 1