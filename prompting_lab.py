# -*- coding: utf-8 -*-
"""Prompting_Lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15nHu_VoYdp6ut99kNtAFQ1KEqaWLPmL4

#Prompting Assignment

## Introduction
Prompting is the process of designing inputs (or 'prompts') for a language model in a way that guides the model to produce the desired type of response. Effective prompting turns unstructured questions or tasks into structured inputs that the model can process to generate accurate and relevant answers.
Prompting in it's core means providing a structured input to an LLM to elicit a specific output or behavior.

## Real-life Use Cases of Prompting
Prompting is crucial in various applications, including but not limited to:
- **Content Generation:** Creating articles, reports, and summaries.
- **Chatbots:** Enhancing user interactions in customer service, virtual assistance, etc.
- **Code Generation:** Assisting developers by generating code snippets and debugging existing code.
- **Educational Tools:** Providing explanations, solving math problems, and generating practice questions.

## Example of prompts for gemini model
"""

# Install the necessary libraries
!pip install transformers

# Import the required modules
from transformers import pipeline

# You have to generate their your own token
import os
os.environ["HF_TOKEN"] = "your_huggingface_token"

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model1 = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

import torch

# Function to generate text from the model
def generate_text(prompt, max_length=150):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

import torch

# Function to generate text from the model
def generate_text1(prompt, max_length=150):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model1.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Bad prompt example: vague and unspecific
bad_prompt = "Tell me about NLP."
bad_response = generate_text(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

# Good prompt example: specific and direct
good_prompt = "Explain the significance of Natural Language Processing in enhancing automated customer support systems today."
good_response = generate_text(good_prompt)
print("\nResponse to Good Prompt:")
print(good_response)

"""## Impact of Bad Prompts on LLM Performance.

The quality of a prompt significantly influences the performance of a language model (LM). Bad prompts can lead to several issues, including poor performance, irrelevant or incorrect information, and potentially biased responses. Understanding these issues helps in crafting better prompts that enhance the utility of LMs in practical applications.

###  Vagueness Leading to Poor Performance
A vague prompt often results in a generic or off-target response, as the model lacks sufficient direction to generate specific information. This can be particularly problematic in professional or technical settings where accuracy and detail are crucial.

*Expected Issue*: The response to this prompt might be overly broad, touching on random aspects of NLP without focusing on current and practical applications, thus not meeting specific user needs.
"""

# Vague prompt about NLP
bad_prompt_vague = "Tell me about NLP."
print("Response to Vague Prompt:")
print(generate_text(bad_prompt_vague))

"""### Incorrect Scope Leading to Irrelevant Information
Prompts that misdirect the focus of the inquiry can lead the LM to generate information that, while correct, is irrelevant to the user's actual needs or intentions.

*Expected Issue*: This prompt may lead the model to give a generic answer on how to install NLP into devices while NLP is more of a concept rather than a library to be installed.

"""

# Incorrectly scoped prompt
bad_prompt_scope = "How is NLP used in devices?"
print("Response to Incorrectly Scoped Prompt:")
print(generate_text(bad_prompt_scope))

"""### Leading Questions Leading to Biased Answers
Prompts that are phrased in a way that implies certain answers can lead to biased or skewed responses. This is particularly risky in sensitive areas such as political content, cultural discussions, or ethical issues.

Expected Issue: This prompt suggests that NLP is failing, which may cause the model to generate a biased critique of NLP technologies, ignoring their successes and advancements.
"""

# Leading prompt suggesting a biased view
bad_prompt_biased = "Why is NLP failing in today's tech?"
print("Response to Biased Prompt:")
print(generate_text(bad_prompt_biased))

"""### Overly Complex Prompts Leading to Confusion
Prompts that are too complex or contain too much information can confuse the model, resulting in garbled or incoherent responses.

*Expected Issue*: The prompt's complexity could overwhelm the model, leading to a response that is fragmented, lacks depth, or is completely irrelevant due to the model's inability to process multiple intricate instructions simultaneously.
"""

# Overly complex prompt
bad_prompt_complex = "Explain how NLP, combined with machine learning and five other AI technologies, is revolutionizing the future of all European tech industries in a comparative analysis to Asian markets."
print("Response to Overly Complex Prompt:")
print(generate_text(bad_prompt_complex))

"""## Tips for Crafting Effective Prompts for Language Models

### Simplicity
Keep your prompts simple and straightforward to avoid confusion and ensure that the model clearly understands the task.

**Explanation:**
Simple prompts help the model focus on the task without being distracted by unnecessary details or complex sentence structures.
"""

simple_prompt = "Summarize the key benefits of using electric vehicles."
print("Response to Simple Prompt:")
print(generate_text(simple_prompt))

"""### 2. Precision in Instruction
Use clear action words like "Write," "Classify," "Summarize," "Translate," "Order," etc., to specify the type of response you expect from the model.


"""

instruction_prompt = "List the steps involved in photosynthesis."
print("Response to Instruction-Based Prompt:")
print(generate_text(instruction_prompt))

"""### 3. Specificity
Be specific about the task and provide detailed information about what you expect from the model.
"""

specific_prompt = "Explain how convolutional neural networks differ from traditional neural networks in image recognition tasks."
print("Response to Specific Prompt:")
print(generate_text(specific_prompt))

"""### Avoid Over-Complication
While details are important, overly clever or complex prompts can lead to imprecise results.


"""

# Overly complex vs. balanced prompt example
complex_prompt = "Discuss how, if at all, quantum computing might revolutionize AI by comparing classical and quantum machine learning algorithms in terms of computational advantages in processing large datasets, specifically in neural network training phases."
balanced_prompt = "Discuss how quantum computing could impact AI, focusing on machine learning algorithms."
print("Response to Overly Complex Prompt:")
print(generate_text(complex_prompt))
print("\nResponse to Balanced Prompt:")
print(generate_text(balanced_prompt))

"""### 5. Positive Instructions
Focus on stating what the model should do rather than what it should not do.

Explanation: Positive instructions are more direct and facilitate clearer understanding and execution by the model.


"""

# Negative vs. positive instruction example
negative_instruction = "Explain blockchain and don't just list the features of blockchain technology."
positive_instruction = "Explain the benefits and applications of blockchain technology."
print("Response to Negative Instruction Prompt:")
print(generate_text(negative_instruction))
print("\nResponse to Positive Instruction Prompt:")
print(generate_text(positive_instruction))

"""## Homework Exercises

Exploring the quality of prompts and how it affects the performance of different Large Language Models (LLMs) across the following domains: common reasoning (e.g., benchmark CoQA dataset), math skills, coding skills, literary skills (e.g., write a romance short in Jane Austen style). Design at least 1  effective and 1 ineffective prompt for each domain, and document the models' responses.

# google/gemma-2b model

### 1. Common Reasoning (CoQA Dataset)
"""

print("Testing domain: Common Reasoning")

# Good prompt: specific and instructive
good_prompt = "Using the CoQA dataset, answer this question: Why is teamwork essential in problem-solving?"
good_response = generate_text(good_prompt)
print("\nGood Prompt:")
print(good_prompt)
print("Response to Good Prompt:")
print(good_response)

# Bad prompt: vague and open-ended
bad_prompt = "Why is teamwork good?"
bad_response = generate_text(bad_prompt)
print("\nBad Prompt:")
print(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

"""### 2. Math Skills

"""

# Good prompt: specific and step-by-step instruction
good_prompt = "Solve for x in the equation: 5x + 3 = 18. Show all steps."
good_response = generate_text(good_prompt)
print("\nGood Prompt:")
print(good_prompt)
print("Response to Good Prompt:")
print(good_response)

# Bad prompt: vague and unspecific
bad_prompt = "Whatâ€™s the answer to 5x + 3 = 18?"
bad_response = generate_text(bad_prompt)
print("\nBad Prompt:")
print(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

"""### 3. Coding Skills"""

print("Testing domain: Coding Skills")

# Good prompt: clear and detailed instruction
good_prompt = "Write a Python function that takes a list of integers and returns the maximum value in the list."
good_response = generate_text(good_prompt)
print("\nGood Prompt:")
print(good_prompt)
print("Response to Good Prompt:")
print(good_response)

# Bad prompt: too vague
bad_prompt = "Write a code."
bad_response = generate_text(bad_prompt)
print("\nBad Prompt:")
print(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

"""### 4. Literary Skills"""

print("Testing domain: Literary Skills")

# Good prompt: specific with style and setting
good_prompt = "Write a romance short story in the style of Jane Austen, focusing on a chance encounter at a countryside ball."
good_response = generate_text(good_prompt)
print("\nGood Prompt:")
print(good_prompt)
print("Response to Good Prompt:")
print(good_response)

# Bad prompt: lacks style or setting direction
bad_prompt = "Write a romantic story."
bad_response = generate_text(bad_prompt)
print("\nBad Prompt:")
print(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

"""# google/flan-t5-base model

## Common Reasoning
"""

print("Testing domain: Common Reasoning")

# Good prompt: specific and instructive
good_prompt = "Using the CoQA dataset, answer this question: Why is teamwork essential in problem-solving?"
good_response = generate_text1(good_prompt)
print("\nGood Prompt:")
print(good_prompt)
print("Response to Good Prompt:")
print(good_response)

# Bad prompt: vague and open-ended
bad_prompt = "Why is teamwork good?"
bad_response = generate_text1(bad_prompt)
print("\nBad Prompt:")
print(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

"""## Math Skills"""

# Good prompt: specific and step-by-step instruction
good_prompt = "Solve for x in the equation: 5x + 3 = 18. Show all steps."
good_response = generate_text1(good_prompt)
print("\nGood Prompt:")
print(good_prompt)
print("Response to Good Prompt:")
print(good_response)

# Bad prompt: vague and unspecific
bad_prompt = "Whatâ€™s the answer to 5x + 3 = 18?"
bad_response = generate_text1(bad_prompt)
print("\nBad Prompt:")
print(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

"""## Coding Skills"""

print("Testing domain: Coding Skills")

# Good prompt: clear and detailed instruction
good_prompt = "Write a Python function that takes a list of integers and returns the maximum value in the list."
good_response = generate_text1(good_prompt)
print("\nGood Prompt:")
print(good_prompt)
print("Response to Good Prompt:")
print(good_response)

# Bad prompt: too vague
bad_prompt = "Write a code."
bad_response = generate_text1(bad_prompt)
print("\nBad Prompt:")
print(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

"""## Literary Skills"""

print("Testing domain: Literary Skills")

# Good prompt: specific with style and setting
good_prompt = "Write a romance short story in the style of Jane Austen, focusing on a chance encounter at a countryside ball."
good_response = generate_text1(good_prompt)
print("\nGood Prompt:")
print(good_prompt)
print("Response to Good Prompt:")
print(good_response)

# Bad prompt: lacks style or setting direction
bad_prompt = "Write a romantic story."
bad_response = generate_text1(bad_prompt)
print("\nBad Prompt:")
print(bad_prompt)
print("Response to Bad Prompt:")
print(bad_response)

"""# Models' Responses

## Gemma-2b

1. Testing domain: Common Reasoning

Good Prompt:
Using the CoQA dataset, answer this question: Why is teamwork essential in problem-solving? Response to Good Prompt: Using the CoQA dataset, answer this question: Why is teamwork essential in problem-solving?
Answer:
Step 1/2 First, we need to understand what the CoQA dataset is. The CoQA dataset is a large-scale question-answering dataset that contains questions and answers from various domains, including science, medicine, and social sciences. Now, let's consider the question: Why is teamwork essential in problem-solving? Teamwork is essential in problem-solving because it allows multiple individuals to work together to identify and solve complex problems. In problem-solving, there are often multiple solutions to a problem, and each individual may have a different approach or perspective. By working together, individuals can combine their knowledge and

Bad Prompt:
Why is teamwork good? Response to Bad Prompt: Why is teamwork good?
The following data were taken from the financial statements of Heston Enterprises Inc. for the current fiscal year. Assuming that long-term investments totaled  $2,100,000  throughout the year and that total assets were  $4,000,000  at the beginning of the year, determine the ratio of liabilities to stockholders' equity. Round to one decimal place.
$\begin{array}{lrrr} \text{Property, plant, and equipment (net) . . . . . . . . . . . } &&& \underline{\underline{$\hspace{5pt}1,600,000}}\ \text{Li

2. Testing domain: Math Skills

Good Prompt:
Solve for x in the equation: 5x + 3 = 18. Show all steps. Response to Good Prompt: Solve for x in the equation: 5x + 3 = 18. Show all steps.
Answer:
Step 1/2 First, we need to isolate the variable x on one side of the equation. To do this, we can subtract 3 from both sides: 5x + 3 - 3 = 18 - 3 Simplifying: 5x = 15
Step 2/2 Next, we need to solve for x by dividing both sides by 5: 5x/5 = 15/5 Simplifying: x = 3 Therefore, the solution to the equation 5x + 3 = 18 is x =

Bad Prompt:
Whatâ€™s the answer to 5x + 3 = 18? Response to Bad Prompt: Whatâ€™s the answer to 5x + 3 = 18?
Answer:
Step 1/2 First, we want to isolate the variable x on one side of the equation. To do this, we can subtract 3 from both sides: 5x + 3 - 3 = 18 - 3 Simplifying: 5x = 15
Step 2/2 Next, we want to solve for x by dividing both sides by 5: 5x/5 = 15/5 Simplifying: x = 3 Therefore, the answer to 5x + 3 = 18 is x = 3.

3. Testing domain: Coding Skills

Good Prompt:
Write a Python function that takes a list of integers and returns the maximum value in the list. Response to Good Prompt: Write a Python function that takes a list of integers and returns the maximum value in the list.
Answer:
def max_value(list): return max(list)

Bad Prompt:
Write a code. Response to Bad Prompt: Write a code.
int[] numbers = {34, 24, 12, 64, 96, 3, 8, 77, 12, 3}; int count = 0; for (int i = 0; i < numbers.length; i++) { if (numbers[i] % 2 == 0) { count = count + 1; } } System.out.println("The number of even numbers is " + count);
What is the difference between a physical property and a chemical property?
A 100-W lightbulb is plugged into a

4. Testing domain: Literary Skills

Good Prompt:
Write a romance short story in the style of Jane Austen, focusing on a chance encounter at a countryside ball. Response to Good Prompt: Write a romance short story in the style of Jane Austen, focusing on a chance encounter at a countryside ball.
Answer:
Step 1/2 Once upon a time, there was a young woman named Jane who lived in a small town in England. She was a beautiful and intelligent girl, but she was also shy and introverted. One day, Jane was invited to a ball by her friend, who was a member of the local gentry. Jane was excited to attend the ball, but she was also nervous about meeting new people and making new friends. As the night went on, Jane danced with different men, but she felt more and more uncomfortable. She was not interested in any of them, and she was not having much

Bad Prompt:
Write a romantic story. Response to Bad Prompt: Write a romantic story.
The story is about a girl who is in love with a boy. The boy is a rich man and the girl is a poor girl. The boy is very rich and he has a lot of money. The girl is very poor and she has no money. The boy is very rich and he has a lot of money. The girl is very poor and she has no money. The boy is very rich and he has a lot of money. The girl is very poor and she has no money. The boy is very rich and he has a lot of money. The girl is very poor and she has no money. The boy is very rich and he has a lot of money. The girl is very poor and

## Flan-t5-base

1. Testing domain: Common Reasoning

Good Prompt:
Using the CoQA dataset, answer this question: Why is teamwork essential in problem-solving?
Response to Good Prompt:
to solve problems

Bad Prompt:
Why is teamwork good?
Response to Bad Prompt:
teamwork is a good way to achieve goals

2. Testing domain: Math Skills
Good Prompt:
Solve for x in the equation: 5x + 3 = 18. Show all steps.
Response to Good Prompt:
-1

Bad Prompt:
Whatâ€™s the answer to 5x + 3 = 18?
Response to Bad Prompt:
-3

3. Testing domain: Coding Skills

Good Prompt:
Write a Python function that takes a list of integers and returns the maximum value in the list.
Response to Good Prompt:
a=list(map(int,input().split())) b=[] for i in range(len(a)): b.append(a[i]) a.sort() b.reverse() if len(b)==1: print(max(b)) else: print(max(b))

Bad Prompt:
Write a code.
Response to Bad Prompt:
a=[] for i in range(len(a)): if a[i][0]==a[i+1][0]: a.append(a[i][1]) else: a.append(a[i][1]) a.append(a[i][2]) a.append(a[i][3]) a.append(a[i][4]) a.append(a[i][5]) a.append(a[i][6]) a.append(a[i]

4. Testing domain: Literary Skills

Good Prompt:
Write a romance short story in the style of Jane Austen, focusing on a chance encounter at a countryside ball.
Response to Good Prompt:
Jane Austen wrote a short story in the style of Jane Austen, focusing on a chance encounter at a countryside ball.

Bad Prompt:
Write a romantic story.
Response to Bad Prompt:
The couple is going to a wedding. They are going to have a wedding. They are going to have a wedding. They are going to have a wedding. They are going to have a wedding.
"""